import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import scipy.sparse as sp
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

# ç„¶åå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.preprocessor import IDSDataPreprocessor
from src.database import DatabaseConnector

class ZeroDayDetector:
    """
    é›¶æ—¥æ”»å‡»æ£€æµ‹å™¨ï¼Œä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹æ£€æµ‹æœªçŸ¥çš„æ”»å‡»æ¨¡å¼
    é›¶æ—¥æ”»å‡»é€šå¸¸è¡¨ç°ä¸ºä¸å·²çŸ¥æ¨¡å¼ä¸åŒçš„ç½‘ç»œè¡Œä¸ºï¼Œè‡ªåŠ¨ç¼–ç å™¨å¯ä»¥å­¦ä¹ æ­£å¸¸æµé‡æ¨¡å¼ï¼Œ
    å¹¶æ£€æµ‹å¼‚å¸¸åå·®ï¼Œè¿™äº›åå·®å¯èƒ½ä»£è¡¨é›¶æ—¥æ”»å‡»ã€‚
    """
    
    def __init__(self, encoding_dim=16, threshold=None):
        """
        åˆå§‹åŒ–é›¶æ—¥æ”»å‡»æ£€æµ‹å™¨
        
        å‚æ•°:
            encoding_dim: ç¼–ç å±‚çš„ç»´åº¦
            threshold: å¼‚å¸¸åˆ†æ•°çš„é˜ˆå€¼ï¼Œå¦‚æœä¸ºNoneï¼Œå°†åœ¨è®­ç»ƒåè‡ªåŠ¨è®¾ç½®
        """
        self.encoding_dim = encoding_dim
        self.threshold = threshold
        self.model = None
        self.scaler = StandardScaler()
        self.input_dim = None
        self.history = None
        self.is_scaler_fitted = False
        
    def _build_model(self, input_dim):
        """
        æ„å»ºè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹
        
        å‚æ•°:
            input_dim: è¾“å…¥ç‰¹å¾çš„ç»´åº¦
        """
        # ä¿å­˜è¾“å…¥ç»´åº¦
        self.input_dim = input_dim
        
        # ç¼–ç å™¨éƒ¨åˆ†
        inputs = tf.keras.layers.Input(shape=(input_dim,))
        encoded = tf.keras.layers.Dense(self.encoding_dim * 4, activation='relu')(inputs)
        encoded = tf.keras.layers.Dropout(0.2)(encoded)
        encoded = tf.keras.layers.Dense(self.encoding_dim * 2, activation='relu')(encoded)
        encoded = tf.keras.layers.Dropout(0.2)(encoded)
        encoded = tf.keras.layers.Dense(self.encoding_dim, activation='relu')(encoded)
        
        # è§£ç å™¨éƒ¨åˆ†
        decoded = tf.keras.layers.Dense(self.encoding_dim * 2, activation='relu')(encoded)
        decoded = tf.keras.layers.Dropout(0.2)(decoded)
        decoded = tf.keras.layers.Dense(self.encoding_dim * 4, activation='relu')(decoded)
        decoded = tf.keras.layers.Dropout(0.2)(decoded)
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨linearæ¿€æ´»å‡½æ•°è€Œä¸æ˜¯sigmoid
        decoded = tf.keras.layers.Dense(input_dim, activation='linear')(decoded)
        
        # æ•´ä¸ªè‡ªåŠ¨ç¼–ç å™¨
        autoencoder = tf.keras.models.Model(inputs, decoded)
        autoencoder.compile(optimizer='adam', loss='mean_squared_error')
        
        # ç¼–ç å™¨æ¨¡å‹ï¼ˆç”¨äºæå–ç‰¹å¾ï¼‰
        encoder = tf.keras.models.Model(inputs, encoded)
        
        self.model = autoencoder
        self.encoder = encoder
        
        return autoencoder
    
    def train(self, X, epochs=50, batch_size=64, validation_split=0.2, save_path=None):
        """
        è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œç»è¿‡é¢„å¤„ç†çš„æ­£å¸¸æµé‡æ•°æ®
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        
        è¿”å›:
            è®­ç»ƒå†å²è®°å½•
        """
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç¨€ç–çŸ©é˜µå¹¶è½¬æ¢
        is_sparse = sp.issparse(X)
        if is_sparse:
            print("æ£€æµ‹åˆ°ç¨€ç–è¾“å…¥çŸ©é˜µï¼Œè½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ")
            X = X.toarray()
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ•°æ®æ ‡å‡†åŒ–
        print("å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ ‡å‡†åŒ–...")
        X_scaled = self.scaler.fit_transform(X)
        self.is_scaler_fitted = True
        print(f"æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]")
        
        # æ„å»ºæ¨¡å‹
        print(f"æ„å»ºè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹: è¾“å…¥ç»´åº¦ {X_scaled.shape[1]}, ç¼–ç ç»´åº¦ {self.encoding_dim}")
        self._build_model(X_scaled.shape[1])
        
        # è®¾ç½®æ—©åœå’Œæ¨¡å‹æ£€æŸ¥ç‚¹
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', 
            patience=10, 
            restore_best_weights=True
        )
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®ï¼‰
        self.history = self.model.fit(
            X_scaled, X_scaled,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
            epochs=epochs,
            batch_size=batch_size,
            shuffle=True,
            validation_split=validation_split,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # è®¡ç®—é‡æ„è¯¯å·®ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®ï¼‰
        reconstructions = self.model.predict(X_scaled)
        train_loss = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
        
        # å¦‚æœæ²¡æœ‰è®¾ç½®é˜ˆå€¼ï¼Œè‡ªåŠ¨è®¾ç½®ä¸ºé‡æ„è¯¯å·®çš„95%åˆ†ä½æ•°
        if self.threshold is None:
            self.threshold = np.percentile(train_loss, 95)
            print(f"è‡ªåŠ¨è®¾ç½®å¼‚å¸¸åˆ†æ•°é˜ˆå€¼ä¸º: {self.threshold:.6f}")
        
        print(f"è®­ç»ƒæ•°æ®é‡å»ºè¯¯å·®ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {train_loss.min():.6f}")
        print(f"  25%åˆ†ä½æ•°: {np.percentile(train_loss, 25):.6f}")
        print(f"  50%åˆ†ä½æ•°: {np.percentile(train_loss, 50):.6f}")
        print(f"  75%åˆ†ä½æ•°: {np.percentile(train_loss, 75):.6f}")
        print(f"  95%åˆ†ä½æ•°: {np.percentile(train_loss, 95):.6f}")
        print(f"  æœ€å¤§å€¼: {train_loss.max():.6f}")
        
        # ä¿å­˜æ¨¡å‹
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            joblib.dump(self, save_path)
            print(f"é›¶æ—¥æ£€æµ‹å™¨æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")
        
        return self.history
    
    def predict(self, X):
        """
        ğŸ”§ ä¿®å¤åçš„é¢„æµ‹æ–¹æ³•ï¼šæ­£ç¡®è®¡ç®—å¼‚å¸¸åˆ†æ•°
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
        
        è¿”å›:
            å¼‚å¸¸åˆ†æ•°æ•°ç»„ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¯èƒ½æ˜¯é›¶æ—¥æ”»å‡»
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        if not self.is_scaler_fitted:
            raise ValueError("æ ‡å‡†åŒ–å™¨å°šæœªè®­ç»ƒ")
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç¨€ç–çŸ©é˜µå¹¶è½¬æ¢
        is_sparse = sp.issparse(X)
        if is_sparse:
            X = X.toarray()
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(X)
        
        # é‡æ„è¾“å…¥å¹¶è®¡ç®—é‡æ„è¯¯å·®
        reconstructions = self.model.predict(X_scaled)
        loss = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„å½’ä¸€åŒ–é€»è¾‘
        if len(loss) > 1:
            min_loss = np.min(loss)
            max_loss = np.max(loss)
            
            # ç¡®ä¿åˆ†æ¯ä¸ä¸ºé›¶
            if max_loss > min_loss:
                # æ ‡å‡†çš„min-maxå½’ä¸€åŒ–åˆ°[0,1]
                anomaly_scores = (loss - min_loss) / (max_loss - min_loss)
            else:
                # æ‰€æœ‰å€¼ç›¸åŒçš„æƒ…å†µ
                anomaly_scores = np.zeros_like(loss)
        else:
            # å•ä¸ªæ ·æœ¬çš„æƒ…å†µï¼šä¸é˜ˆå€¼æ¯”è¾ƒ
            if loss[0] > self.threshold:
                anomaly_scores = np.array([1.0])
            else:
                anomaly_scores = np.array([loss[0] / self.threshold])
        
        return anomaly_scores
    
    def predict_raw_errors(self, X):
        """
        è¿”å›åŸå§‹é‡å»ºè¯¯å·®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
        
        è¿”å›:
            åŸå§‹é‡å»ºè¯¯å·®æ•°ç»„
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        if not self.is_scaler_fitted:
            raise ValueError("æ ‡å‡†åŒ–å™¨å°šæœªè®­ç»ƒ")
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç¨€ç–çŸ©é˜µå¹¶è½¬æ¢
        is_sparse = sp.issparse(X)
        if is_sparse:
            X = X.toarray()
        
        # ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(X)
        
        # é‡æ„è¾“å…¥å¹¶è®¡ç®—é‡æ„è¯¯å·®
        reconstructions = self.model.predict(X_scaled)
        loss = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
        
        return loss
    
    def is_zero_day(self, X, threshold=None):
        """
        åˆ¤æ–­æ ·æœ¬æ˜¯å¦å¯èƒ½æ˜¯é›¶æ—¥æ”»å‡»
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
            threshold: å¼‚å¸¸åˆ†æ•°é˜ˆå€¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.threshold
        
        è¿”å›:
            å¸ƒå°”æ•°ç»„ï¼ŒTrueè¡¨ç¤ºå¯èƒ½æ˜¯é›¶æ—¥æ”»å‡»
        """
        if threshold is None:
            threshold = self.threshold
        
        # ä½¿ç”¨åŸå§‹é‡å»ºè¯¯å·®ä¸è®­ç»ƒé˜ˆå€¼æ¯”è¾ƒ
        raw_errors = self.predict_raw_errors(X)
        return raw_errors > threshold
    
    def encode_features(self, X):
        """
        ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾è¡¨ç¤º
        
        å‚æ•°:
            X: è¾“å…¥ç‰¹å¾çŸ©é˜µ
        
        è¿”å›:
            ç¼–ç åçš„ç‰¹å¾
        """
        if self.encoder is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç¨€ç–çŸ©é˜µå¹¶è½¬æ¢
        is_sparse = sp.issparse(X)
        if is_sparse:
            X = X.toarray()
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†åŒ–
        if self.is_scaler_fitted:
            X_scaled = self.scaler.transform(X)
            return self.encoder.predict(X_scaled)
        else:
            return self.encoder.predict(X)
    
    def visualize_loss(self, save_path=None):
        """
        å¯è§†åŒ–è®­ç»ƒå’ŒéªŒè¯æŸå¤±
        
        å‚æ•°:
            save_path: å›¾åƒä¿å­˜è·¯å¾„
        """
        try:
            if self.history is None:
                raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
            plt.figure(figsize=(10, 6))
            plt.plot(self.history.history['loss'], label='è®­ç»ƒæŸå¤±')
            plt.plot(self.history.history['val_loss'], label='éªŒè¯æŸå¤±')
            plt.title('è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒæŸå¤±')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('æŸå¤±')
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.7)
            
            if save_path:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                plt.savefig(save_path)
        except Exception as e:
            print(f"å¯è§†åŒ–æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ç¡®ä¿å§‹ç»ˆå…³é—­å›¾å½¢
            plt.close('all')
    
    def visualize_reconstruction(self, X, n_samples=5, save_path=None):
        """
        å¯è§†åŒ–åŸå§‹è¾“å…¥å’Œé‡æ„è¾“å‡ºçš„æ¯”è¾ƒ
        
        å‚æ•°:
            X: è¾“å…¥ç‰¹å¾çŸ©é˜µ
            n_samples: è¦å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡
            save_path: å›¾åƒä¿å­˜è·¯å¾„
        """
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç¨€ç–çŸ©é˜µå¹¶è½¬æ¢
            is_sparse = sp.issparse(X)
            if is_sparse:
                X = X.toarray()
            
            # é€‰æ‹©æ ·æœ¬å¹¶è·å–é‡æ„
            indices = np.random.choice(X.shape[0], min(n_samples, X.shape[0]), replace=False)
            X_sample = X[indices]
            X_reconstructed = self.model.predict(X_sample)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é‡æ„è¯¯å·®
            reconstruction_errors = np.mean(np.power(X_sample - X_reconstructed, 2), axis=1)
            
            # å¯è§†åŒ–
            plt.figure(figsize=(15, 5 * n_samples))
            for i in range(len(indices)):
                # åŸå§‹æ•°æ®
                plt.subplot(n_samples, 2, i*2 + 1)
                plt.plot(X_sample[i])
                plt.title(f'åŸå§‹ç‰¹å¾ (æ ·æœ¬ {indices[i]})')
                plt.grid(True, linestyle='--', alpha=0.7)
                
                # é‡æ„æ•°æ®
                plt.subplot(n_samples, 2, i*2 + 2)
                plt.plot(X_reconstructed[i])
                plt.title(f'é‡æ„ç‰¹å¾ (è¯¯å·®: {reconstruction_errors[i]:.4f})')
                plt.grid(True, linestyle='--', alpha=0.7)
            
            plt.tight_layout()
            
            if save_path:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                plt.savefig(save_path)
        except Exception as e:
            print(f"å¯è§†åŒ–é‡æ„æ•ˆæœæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ç¡®ä¿å§‹ç»ˆå…³é—­å›¾å½¢
            plt.close('all')
    
    @classmethod
    def load(cls, path="models/zero_day_detector.joblib"):
        """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹"""
        return joblib.load(path)


def train_zero_day_detector(db_connector, epochs=50, save_path="models/zero_day_detector.joblib"):
    """
    è®­ç»ƒé›¶æ—¥æ”»å‡»æ£€æµ‹å™¨
    
    å‚æ•°:
        db_connector: DatabaseConnectorå®ä¾‹
        epochs: è®­ç»ƒè½®æ•°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    
    è¿”å›:
        è®­ç»ƒå¥½çš„ZeroDayDetectorå®ä¾‹
    """
    print("\n" + "="*80)
    print("é›¶æ—¥æ”»å‡»æ£€æµ‹å™¨è®­ç»ƒ")
    print("="*80)
    
    # è·å–å½“å‰è„šæœ¬ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(os.path.dirname(current_dir), "models")
    os.makedirs(models_dir, exist_ok=True)
    
    # åŠ è½½é¢„å¤„ç†å™¨
    preprocessor_path = os.path.join(models_dir, "preprocessor.joblib")
    if not os.path.exists(preprocessor_path):
        print(f"é”™è¯¯: é¢„å¤„ç†å™¨æ–‡ä»¶ä¸å­˜åœ¨: {preprocessor_path}")
        print("è¯·å…ˆè¿è¡Œtrain_baseline.pyè®­ç»ƒåŸºçº¿æ¨¡å‹")
        return None
        
    try:
        preprocessor = IDSDataPreprocessor.load(preprocessor_path)
        print(f"æˆåŠŸåŠ è½½é¢„å¤„ç†å™¨: {preprocessor_path}")
    except Exception as e:
        print(f"åŠ è½½é¢„å¤„ç†å™¨å¤±è´¥: {e}")
        return None
    
    # è·å–åŸºçº¿å‘Šè­¦æ•°æ®ï¼ˆæ­£å¸¸æµé‡ï¼‰
    baseline_df = db_connector.get_baseline_alerts()
    
    if baseline_df is None or len(baseline_df) == 0:
        print("é”™è¯¯: æ— æ³•è·å–åŸºçº¿æ•°æ®æˆ–åŸºçº¿æ•°æ®ä¸ºç©º")
        print("è¯·å…ˆè¿è¡Œfilter_alerts.pyç§¯ç´¯ä¸€äº›åŸºçº¿æ•°æ®")
        
        # æç¤ºç”¨æˆ·æ˜¯å¦æ‰‹åŠ¨åˆ›å»ºä¸€äº›åŸºçº¿æ•°æ®
        print("\næ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹SQLå‘½ä»¤æ‰‹åŠ¨æ·»åŠ ä¸€äº›åŸºçº¿æ•°æ®:")
        print("INSERT INTO baseline_alerts SELECT * FROM ids_ai WHERE threat_level < 2 LIMIT 50;")
        return None
    
    print(f"è·å–åˆ° {len(baseline_df)} æ¡åŸºçº¿æ•°æ®ç”¨äºè®­ç»ƒ")
    
    # é¢„å¤„ç†åŸºçº¿æ•°æ®
    X_baseline, _ = preprocessor.preprocess(baseline_df, fit=False)
    
    # åˆ›å»ºé›¶æ—¥æ£€æµ‹å™¨æ¨¡å‹
    detector = ZeroDayDetector(encoding_dim=32)
    
    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒé›¶æ—¥æ”»å‡»æ£€æµ‹å™¨...")
    history = detector.train(X_baseline, epochs=epochs, save_path=save_path)
    
    # å¯è§†åŒ–è®­ç»ƒæŸå¤±
    loss_path = os.path.join(models_dir, "zero_day_loss.png")
    detector.visualize_loss(save_path=loss_path)
    
    # å¯è§†åŒ–é‡æ„æ•ˆæœ
    if sp.issparse(X_baseline):
        X_sample = X_baseline[:10].toarray()
    else:
        X_sample = X_baseline[:10]
    
    reconstruction_path = os.path.join(models_dir, "zero_day_reconstruction.png")
    detector.visualize_reconstruction(X_sample, n_samples=5, save_path=reconstruction_path)
    
    print("\né›¶æ—¥æ”»å‡»æ£€æµ‹å™¨è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    
    return detector


def main():
    """é›¶æ—¥æ£€æµ‹å™¨è®­ç»ƒä¸»å‡½æ•°"""
    # è·å–å½“å‰è„šæœ¬ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(os.path.dirname(current_dir), "config", "config.env")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    print(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    
    # è·å–æ•°æ®åº“è¿æ¥
    db = DatabaseConnector(config_path)
    
    # è®­ç»ƒé›¶æ—¥æ£€æµ‹å™¨
    save_path = os.path.join(os.path.dirname(current_dir), "models", "zero_day_detector.joblib")
    train_zero_day_detector(db, epochs=50, save_path=save_path)


if __name__ == "__main__":
    main() 